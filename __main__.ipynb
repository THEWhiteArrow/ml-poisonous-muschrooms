{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from typing import Tuple, List, TypedDict, cast\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGBOOK:\n",
    "\n",
    "- Obervations:\n",
    "    - There is a lot of missing values in the dataset\n",
    "    - In columns sometimes there is additional irrelevant information (like the name of the column or random gibberish)\n",
    "\n",
    "- Ideas:\n",
    "    - Why don't we try to implement a solution that would predict only for rows that have all the values, rows without one certain column, rows without two certain columns, and so on?\n",
    "    - Why don't we make models for each of the case and then compare them to the one that predicts for all rows?\n",
    "    If we use a model that would output the probability instead of the class, we could ensamble the predictions by getting the predictions from different models, comparing the probabilities vs accuracy, getting the weights and ensambling the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train = pd.read_csv('./data/train.csv')\n",
    "    test = pd.read_csv('./data/test.csv')\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(name: str | None = None) -> logging.Logger:\n",
    "    \"\"\"A function to setup the logger\n",
    "\n",
    "    Args:\n",
    "        name (str | None, optional): Logger name. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        logging.Logger: Configured logger\n",
    "    \"\"\"\n",
    "\n",
    "    file_handler = logging.FileHandler(\"logs.log\", mode=\"a\")\n",
    "    stream_handler = logging.StreamHandler(stream=sys.stdout)\n",
    "\n",
    "    logger = logging.getLogger(name or __name__)\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.WARNING,\n",
    "        format=\"%(levelname)s %(name)s %(asctime)s | %(message)s\",\n",
    "        datefmt=\"%H:%M:%S\",\n",
    "        handlers=[file_handler, stream_handler],\n",
    "    )\n",
    "\n",
    "    \n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    return logger\n",
    "\n",
    "logger = setup_logger(\"Main\")\n",
    "logger.info(\"Logger setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ColumnTypes(TypedDict):\n",
    "    \"\"\"\n",
    "    A class to store the features and target columns of the dataset.\n",
    "\n",
    "    Attributes:\n",
    "    - ids: List[str]\n",
    "    - labels: List[str]\n",
    "    - numerical: List[str]\n",
    "    - categorical: List[str]\n",
    "    - targets: List[str]\n",
    "    \"\"\"\n",
    "\n",
    "    ids: List[str]\n",
    "    \"\"\"An array of the columns that are used as the ID of the dataset.\"\"\"\n",
    "    labels: List[str]\n",
    "    \"\"\"An array of the columns that are used as the labels of the dataset. \n",
    "    Label encoding is to be used when there is a sequential correlation between the labels.\"\"\"\n",
    "\n",
    "    numerical: List[str]\n",
    "    \"\"\"An array of the columns that are numerical in nature.\"\"\"\n",
    "    categorical: List[str]\n",
    "    \"\"\"An array of the columns that are categorical in nature.\"\"\"\n",
    "    targets: List[str]\n",
    "    \"\"\"An array of the columns that are the target of the dataset.\"\"\"\n",
    "\n",
    "\n",
    "def get_columns_types() -> ColumnTypes:\n",
    "    \"\"\"A function that defines the types of columns in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        ColumnTypes: ColumnTypes object that contains the columns of the dataset.\n",
    "    \"\"\"\n",
    "    return ColumnTypes(\n",
    "        ids=[\"id\"],\n",
    "        targets=[\"class\"],\n",
    "        labels=[\"gill-spacing\"],\n",
    "        numerical=[\n",
    "            \"cap-diameter\",\n",
    "            \"stem-height\",\n",
    "            \"stem-width\",\n",
    "        ],\n",
    "        categorical=[\n",
    "            \"cap-shape\",\n",
    "            \"cap-surface\",\n",
    "            \"cap-color\",\n",
    "            \"does-bruise-or-bleed\",\n",
    "            \"gill-attachment\",\n",
    "            \"gill-color\",\n",
    "            \"stem-root\",\n",
    "            \"stem-surface\",\n",
    "            \"stem-color\",\n",
    "            \"veil-type\",\n",
    "            \"veil-color\",\n",
    "            \"has-ring\",\n",
    "            \"ring-type\",\n",
    "            \"spore-print-color\",\n",
    "            \"habitat\",\n",
    "            \"season\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ColumnSet:\n",
    "    \"\"\"A class to store the information about the features of the dataset.\"\"\"\n",
    "\n",
    "    is_optional: bool\n",
    "    \"\"\"A boolean value to determine if the column list is optional.\"\"\"\n",
    "    columns: List[str]\n",
    "    \"\"\"An array of the columns that are to be used in the dataset.\"\"\"\n",
    "\n",
    "\n",
    "def get_column_sets() -> List[ColumnSet]:\n",
    "    \"\"\"A function that defines the possible column combinations for the dataset.\n",
    "    It will create a list of ColumnSet objects that contain the possible column combinations.\n",
    "\n",
    "    Returns:\n",
    "        List[ColumnSet]: A list of ColumnSet objects that contain the possible column combinations.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        ColumnSet(is_optional=True, columns=[\"does-bruise-or-bleed\"]),\n",
    "        ColumnSet(is_optional=False, columns=[\"stem_height\", \"stem_width\"]),\n",
    "        ColumnSet(\n",
    "            is_optional=True,\n",
    "            columns=[\"cap-diameter\", \"cap-shape\", \"cap-surface\", \"cap-color\"],\n",
    "        ),\n",
    "        ColumnSet(\n",
    "            is_optional=True, columns=[\"gill-spacing\", \"gill-attachment\", \"gill-color\"]\n",
    "        ),\n",
    "        ColumnSet(\n",
    "            is_optional=True, columns=[\"stem-root\", \"stem-surface\", \"stem-color\"]\n",
    "        ),\n",
    "        ColumnSet(is_optional=True, columns=[\"veil-type\", \"veil-color\"]),\n",
    "        ColumnSet(is_optional=True, columns=[\"has-ring\", \"ring-type\"]),\n",
    "        ColumnSet(is_optional=True, columns=[\"spore-print-color\", \"habitat\", \"season\"]),\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_column_transformer(\n",
    "    column_types: ColumnTypes, categorical_outliers_frequency_limit: float\n",
    ") -> Pipeline:\n",
    "    \"\"\"A function that creates a pipeline object that contains the transformers to be used for the columns.\n",
    "\n",
    "    Returns:\n",
    "        Pipeline: A pipeline object that contains the transformers to be used for the columns.\n",
    "    \"\"\"\n",
    "\n",
    "    def label_encode(x: pd.DataFrame) -> pd.DataFrame:\n",
    "        logger.info(\"Label encoding started\")\n",
    "        for column in x.columns:\n",
    "            if column in [*column_types[\"labels\"], *column_types[\"targets\"] ]:\n",
    "                x[column] = pd.Series(LabelEncoder().fit_transform(x[column])).rename(\n",
    "                    column\n",
    "                )\n",
    "\n",
    "        logger.info(\"Label encoding complete\")\n",
    "        return x\n",
    "\n",
    "    def clean_categorical_columns(x: pd.DataFrame) -> pd.DataFrame:\n",
    "        logger.info(\"Cleaning categorical columns started\")\n",
    "        logger.info(\n",
    "            f\"Outliers frequency limit is {categorical_outliers_frequency_limit}\"\n",
    "        )\n",
    "        for column in x.columns:\n",
    "            if column in [*column_types[\"labels\"], *column_types[\"categorical\"]]:\n",
    "\n",
    "                value_counts = x[column].value_counts().to_frame()\n",
    "                sum_value_counts = value_counts[\"count\"].sum()\n",
    "\n",
    "                outliers = cast(\n",
    "                    pd.DataFrame,\n",
    "                    value_counts[\n",
    "                        value_counts[\"count\"]\n",
    "                        < sum_value_counts * categorical_outliers_frequency_limit\n",
    "                    ],\n",
    "                ).index.to_list()\n",
    "                logger.info(f\"Outliers for column '{column}' are {outliers}\")\n",
    "                x[column] = (\n",
    "                    pd.Series(\n",
    "                        x[column].apply(\n",
    "                            lambda el: el if el not in outliers else \"gibberish\"\n",
    "                        )\n",
    "                    )\n",
    "                    .rename(column)\n",
    "                    .fillna(\"na\")\n",
    "                    .astype(\"category\")\n",
    "                )\n",
    "\n",
    "                cleaned_series = x[column]\n",
    "                x[column] = cleaned_series\n",
    "\n",
    "        logger.info(\"Cleaning categorical columns complete\")\n",
    "        return x\n",
    "\n",
    "    def clean_numerical_columns(x: pd.DataFrame) -> pd.DataFrame:\n",
    "        logger.info(\"Cleaning numerical columns started\")\n",
    "        for column in x.columns:\n",
    "            if column in column_types[\"numerical\"]:\n",
    "                n_nans = x[column].isna().sum()\n",
    "                if n_nans > 0:\n",
    "                    logger.info(\n",
    "                        f\"Column '{column}' has {n_nans} NaNs. Filling with mean {x[column].mean()}\"\n",
    "                    )\n",
    "                    x[column] = x[column].fillna(x[column].mean())\n",
    "\n",
    "                x[column] = x[column].astype(\"float16\")\n",
    "\n",
    "        logger.info(\"Cleaning numerical columns complete\")\n",
    "        return x\n",
    "\n",
    "    categorical_column_cleaner = FunctionTransformer(\n",
    "        clean_categorical_columns,\n",
    "    )\n",
    "    numerical_column_cleaner = FunctionTransformer(clean_numerical_columns)\n",
    "\n",
    "    label_encoder_setup = FunctionTransformer(label_encode)\n",
    "\n",
    "    column_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"numerical\", StandardScaler(), column_types[\"numerical\"]),\n",
    "            (\n",
    "                \"categorical\",\n",
    "                OneHotEncoder(drop=\"first\", sparse_output=False),\n",
    "                column_types[\"categorical\"],\n",
    "            ),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "    logger.info(\"Column transformer created\")\n",
    "    transform_pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"categorical_column_cleaner\", categorical_column_cleaner),\n",
    "            (\"numerical_column_cleaner\", numerical_column_cleaner),\n",
    "            (\"label_transformer\", label_encoder_setup),\n",
    "            (\"column_transformer\", column_transformer),\n",
    "        ],\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    return transform_pipeline\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ColumnCreator:\n",
    "    \"\"\"A class to store the information about the columns to be created.\"\"\"\n",
    "\n",
    "    columns_sets: List[ColumnSet]\n",
    "    \"\"\"An array of the columns that are to be used in the dataset.\"\"\"\n",
    "    column_types: ColumnTypes\n",
    "    \"\"\"A ColumnTypes object that contains the columns of the dataset.\"\"\"\n",
    "    pipeline: Pipeline\n",
    "    \"\"\"A pipeline object that contains the transformers to be used for the columns.\"\"\"\n",
    "\n",
    "    def get_possible_columns_configs(self) -> List[List[str]]:\n",
    "        \"\"\"A function that returns the possible column configurations for the dataset.\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: A list of lists that contain the possible column configurations.\n",
    "        \"\"\"\n",
    "        optional_columns_sets = [\n",
    "            column_set for column_set in self.columns_sets if column_set.is_optional\n",
    "        ]\n",
    "        mandatory_columns_sets = [\n",
    "            column_set for column_set in self.columns_sets if not column_set.is_optional\n",
    "        ]\n",
    "\n",
    "        if len(optional_columns_sets) > 10:\n",
    "            logger.warning(\n",
    "                f\"The number of optional columns sets is too high (more than 10) - {len(optional_columns_sets)}\"\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\n",
    "                f\"The number of optional columns sets is {len(optional_columns_sets)}\"\n",
    "            )\n",
    "\n",
    "        bitmap = 2 ** len(optional_columns_sets) - 1\n",
    "\n",
    "        possible_columns_configs: List[List[str]] = []\n",
    "        mandatory_columns: List[str] = [\n",
    "            column\n",
    "            for mandatory_set in mandatory_columns_sets\n",
    "            for column in mandatory_set.columns\n",
    "        ]\n",
    "        if len(mandatory_columns) > 0:\n",
    "            possible_columns_configs.append(mandatory_columns)\n",
    "            \n",
    "        for i in range(1, bitmap + 1):\n",
    "            columns_config: List[List[str]] = [*mandatory_columns]\n",
    "            for j in range(len(optional_columns_sets)):\n",
    "                if i & (1 << j):\n",
    "                    columns_config.extend(optional_columns_sets[j].columns)\n",
    "\n",
    "            possible_columns_configs.append(columns_config)\n",
    "\n",
    "        return possible_columns_configs\n",
    "\n",
    "    def get_transformed_columns_out_names(self) -> List[str]:\n",
    "        \"\"\"A function that returns the names of the columns after transformation.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of strings that contain the names of the columns after transformation.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.pipeline.named_steps[\"column_transformer\"].get_feature_names_out()\n",
    "\n",
    "    def transform_columns(self, data: pd.DataFrame, fit: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"A function that transforms the columns of the dataset.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The dataset to be transformed.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The transformed dataset.\n",
    "        \"\"\"\n",
    "        if fit is True:\n",
    "            raw_transformed_data = self.pipeline.fit_transform(data.copy())\n",
    "        else:\n",
    "            raw_transformed_data = self.pipeline.transform(data.copy())\n",
    "\n",
    "        transformed_data_df = pd.DataFrame(\n",
    "            data=raw_transformed_data,\n",
    "            columns=self.get_transformed_columns_out_names(),\n",
    "        )\n",
    "\n",
    "        return transformed_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cv(column_set : List[str], pipeline : Pipeline, data: pd.DataFrame, n_splits: int = 5) -> float:\n",
    "    \"\"\"A function that trains a model using cross-validation.\n",
    "\n",
    "    Args:\n",
    "        column_set (List[str]): The columns to be used in the dataset.\n",
    "        model: The model to be used for training.\n",
    "        data (pd.DataFrame): The dataset to be used for training.\n",
    "        n_splits (int, optional): The number of splits to be used for cross-validation. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean accuracy of the model.\n",
    "    \"\"\"\n",
    "    data_columns = data.columns.to_list()\n",
    "\n",
    "    matched_columns : List[str] = []\n",
    "\n",
    "    for transformed_column_name in data_columns:\n",
    "        for column_name in column_set:\n",
    "            if column_name in transformed_column_name:\n",
    "                matched_columns.append(transformed_column_name)\n",
    "    \n",
    "    y = data[[*column_types[\"targets\"]]].squeeze()\n",
    "    X = data[matched_columns]\n",
    "\n",
    "    scores = cross_val_score(pipeline, X, y, cv=n_splits, scoring=\"accuracy\")\n",
    "\n",
    "    return scores.mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sets = get_column_sets()\n",
    "column_types = get_columns_types()\n",
    "column_transformer = get_column_transformer(column_types=column_types, categorical_outliers_frequency_limit=0.01)\n",
    "\n",
    "column_creator = ColumnCreator(\n",
    "    columns_sets=column_sets,\n",
    "    column_types=column_types,\n",
    "    pipeline=column_transformer\n",
    ")\n",
    "possible_columns_configs = column_creator.get_possible_columns_configs()\n",
    "transformed_data = column_creator.transform_columns(train.head(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train.head())\n",
    "display(transformed_data.head())\n",
    "# for i, columns_config in enumerate(possible_columns_configs):\n",
    "#     logger.info(f\"Columns config {i+1}/{len(possible_columns_configs)}: {columns_config}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
