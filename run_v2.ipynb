{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from typing import Tuple, List, TypedDict, cast\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from joblib import Memory\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train = pd.read_csv('./data/train.csv')\n",
    "    test = pd.read_csv('./data/test.csv')\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(name: str | None = None) -> logging.Logger:\n",
    "    \"\"\"A function to setup the logger\n",
    "\n",
    "    Args:\n",
    "        name (str | None, optional): Logger name. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        logging.Logger: Configured logger\n",
    "    \"\"\"\n",
    "\n",
    "    file_handler = logging.FileHandler(\"logs.log\", mode=\"a\")\n",
    "    stream_handler = logging.StreamHandler(stream=sys.stdout)\n",
    "\n",
    "    logger = logging.getLogger(name or __name__)\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.WARNING,\n",
    "        format=\"%(levelname)s %(name)s %(asctime)s | %(message)s\",\n",
    "        datefmt=\"%H:%M:%S\",\n",
    "        handlers=[file_handler, stream_handler],\n",
    "    )\n",
    "\n",
    "    \n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    return logger\n",
    "\n",
    "logger = setup_logger(\"Main\")\n",
    "logger.info(\"Logger setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ColumnTypes(TypedDict):\n",
    "    \"\"\"\n",
    "    A class to store the features and target columns of the dataset.\n",
    "\n",
    "    Attributes:\n",
    "    - ids: List[str]\n",
    "    - labels: List[str]\n",
    "    - numerical: List[str]\n",
    "    - categorical: List[str]\n",
    "    - targets: List[str]\n",
    "    \"\"\"\n",
    "\n",
    "    ids: List[str]\n",
    "    \"\"\"An array of the columns that are used as the ID of the dataset.\"\"\"\n",
    "    labels: List[str]\n",
    "    \"\"\"An array of the columns that are used as the labels of the dataset. \n",
    "    Label encoding is to be used when there is a sequential correlation between the labels.\"\"\"\n",
    "\n",
    "    numerical: List[str]\n",
    "    \"\"\"An array of the columns that are numerical in nature.\"\"\"\n",
    "    categorical: List[str]\n",
    "    \"\"\"An array of the columns that are categorical in nature.\"\"\"\n",
    "    targets: List[str]\n",
    "    \"\"\"An array of the columns that are the target of the dataset.\"\"\"   \n",
    "\n",
    "def take_column_types_subset(column_types: ColumnTypes, columns: List[str], keep_ids_targets: bool = False) -> ColumnTypes:\n",
    "    \"\"\"A function that returns a subset of the ColumnTypes object.\n",
    "\n",
    "    Args:\n",
    "        columns (List[str]): A list of columns to be taken from the ColumnTypes object.\n",
    "\n",
    "    Returns:\n",
    "        ColumnTypes: A ColumnTypes object that contains the subset of columns.\n",
    "    \"\"\"\n",
    "    return ColumnTypes(\n",
    "        ids=[col for col in columns if col in column_types[\"ids\"]] if keep_ids_targets is False else column_types[\"ids\"],\n",
    "        labels=[col for col in columns if col in column_types[\"labels\"]],\n",
    "        numerical=[col for col in columns if col in column_types[\"numerical\"]],\n",
    "        categorical=[col for col in columns if col in column_types[\"categorical\"]],\n",
    "        targets=[col for col in columns if col in column_types[\"targets\"]] if keep_ids_targets is False else column_types[\"targets\"],\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def get_columns_types() -> ColumnTypes:\n",
    "    \"\"\"A function that defines the types of columns in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        ColumnTypes: ColumnTypes object that contains the columns of the dataset.\n",
    "    \"\"\"\n",
    "    return ColumnTypes(\n",
    "        ids=[\"id\"],\n",
    "        targets=[\"class\"],\n",
    "        labels=[\"gill-spacing\"],\n",
    "        numerical=[\n",
    "            \"cap-diameter\",\n",
    "            \"stem-height\",\n",
    "            \"stem-width\",\n",
    "        ],\n",
    "        categorical=[\n",
    "            \"cap-shape\",\n",
    "            \"cap-surface\",\n",
    "            \"cap-color\",\n",
    "            \"does-bruise-or-bleed\",\n",
    "            \"gill-attachment\",\n",
    "            \"gill-color\",\n",
    "            \"stem-root\",\n",
    "            \"stem-surface\",\n",
    "            \"stem-color\",\n",
    "            \"veil-type\",\n",
    "            \"veil-color\",\n",
    "            \"has-ring\",\n",
    "            \"ring-type\",\n",
    "            \"spore-print-color\",\n",
    "            \"habitat\",\n",
    "            \"season\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class ColumnSet:\n",
    "    \"\"\"A class to store the information about the features of the dataset.\"\"\"\n",
    "\n",
    "    is_optional: bool\n",
    "    \"\"\"A boolean value to determine if the column list is optional.\"\"\"\n",
    "    columns: List[str]\n",
    "    \"\"\"An array of the columns that are to be used in the dataset.\"\"\"\n",
    "\n",
    "\n",
    "def get_column_sets() -> List[ColumnSet]:\n",
    "    \"\"\"A function that defines the possible column combinations for the dataset.\n",
    "    It will create a list of ColumnSet objects that contain the possible column combinations.\n",
    "\n",
    "    Returns:\n",
    "        List[ColumnSet]: A list of ColumnSet objects that contain the possible column combinations.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        ColumnSet(is_optional=True, columns=[\"does-bruise-or-bleed\"]),\n",
    "        ColumnSet(is_optional=False, columns=[\"stem_height\", \"stem_width\"]),\n",
    "        ColumnSet(\n",
    "            is_optional=True,\n",
    "            columns=[\"cap-diameter\", \"cap-shape\", \"cap-surface\", \"cap-color\"],\n",
    "        ),\n",
    "        ColumnSet(\n",
    "            is_optional=True, columns=[\"gill-spacing\", \"gill-attachment\", \"gill-color\"]\n",
    "        ),\n",
    "        ColumnSet(\n",
    "            is_optional=True, columns=[\"stem-root\", \"stem-surface\", \"stem-color\"]\n",
    "        ),\n",
    "        ColumnSet(is_optional=True, columns=[\"veil-type\", \"veil-color\"]),\n",
    "        ColumnSet(is_optional=True, columns=[\"has-ring\", \"ring-type\"]),\n",
    "        ColumnSet(is_optional=True, columns=[\"spore-print-color\", \"habitat\", \"season\"]),\n",
    "    ]\n",
    "\n",
    "def get_possible_columns_configs(column_sets : List[ColumnSet]) -> List[List[str]]:\n",
    "    \"\"\"A function that returns the possible column configurations for the dataset.\n",
    "\n",
    "    Returns:\n",
    "        List[List[str]]: A list of lists that contain the possible column configurations.\n",
    "    \"\"\"\n",
    "    optional_columns_sets = [\n",
    "        column_set for column_set in column_sets if column_set.is_optional\n",
    "    ]\n",
    "    mandatory_columns_sets = [\n",
    "        column_set for column_set in column_sets if not column_set.is_optional\n",
    "    ]\n",
    "\n",
    "    if len(optional_columns_sets) > 10:\n",
    "        logger.warning(\n",
    "            f\"The number of optional columns sets is too high (more than 10) - {len(optional_columns_sets)}\"\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\n",
    "            f\"The number of optional columns sets is {len(optional_columns_sets)}\"\n",
    "        )\n",
    "\n",
    "    bitmap = 2 ** len(optional_columns_sets) - 1\n",
    "\n",
    "    possible_columns_configs: List[List[str]] = []\n",
    "    mandatory_columns: List[str] = [\n",
    "        column\n",
    "        for mandatory_set in mandatory_columns_sets\n",
    "        for column in mandatory_set.columns\n",
    "    ]\n",
    "    if len(mandatory_columns) > 0:\n",
    "        possible_columns_configs.append(mandatory_columns)\n",
    "        \n",
    "    for i in range(1, bitmap + 1):\n",
    "        columns_config: List[List[str]] = [*mandatory_columns]\n",
    "        for j in range(len(optional_columns_sets)):\n",
    "            if i & (1 << j):\n",
    "                columns_config.extend(optional_columns_sets[j].columns)\n",
    "\n",
    "        possible_columns_configs.append(columns_config)\n",
    "\n",
    "    return possible_columns_configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(X: pd.DataFrame, columns: List[str], name:str) -> pd.DataFrame:\n",
    "    logger.info(f\"Pipeline {name}: Encoding labels...\")\n",
    "    for column in columns:\n",
    "        X.loc[:, column] = pd.Series(LabelEncoder().fit_transform(X[column])).rename(column)\n",
    "\n",
    "    logger.info(f\"Pipeline {name}: Encoding labels complete\")\n",
    "    return X\n",
    "\n",
    "def clean_categorical(X: pd.DataFrame, columns: List[str], name:str) -> pd.DataFrame:\n",
    "    categorical_outliers_frequency_limit = 0.01\n",
    "    logger.info(f\"Pipeline {name}: Cleaning categorical data...\")\n",
    "    logger.info(\n",
    "        f\"Outliers frequency limit is {categorical_outliers_frequency_limit}\"\n",
    "    )\n",
    "\n",
    "    for column in columns:\n",
    "        value_counts = X[column].value_counts().to_frame()\n",
    "        sum_value_counts = value_counts[\"count\"].sum()\n",
    "\n",
    "        outliers = cast(\n",
    "                pd.DataFrame,\n",
    "                value_counts[\n",
    "                    value_counts[\"count\"]\n",
    "                    < sum_value_counts * categorical_outliers_frequency_limit\n",
    "                ],\n",
    "            ).index.to_list()\n",
    "        logger.info(f\"Outliers for column '{column}' are {outliers}\")\n",
    "\n",
    "        X[column] = (\n",
    "            pd.Series(\n",
    "                X[column].apply(\n",
    "                    lambda el: el if el not in outliers else \"gibberish\"\n",
    "                )\n",
    "            )\n",
    "            .rename(column)\n",
    "            .fillna(\"na\")\n",
    "            .astype(\"category\")\n",
    "        )\n",
    "\n",
    "    logger.info(f\"Pipeline {name}: Cleaning categorical data complete\")\n",
    "    return X\n",
    "\n",
    "def clean_numerical(X: pd.DataFrame, columns: List[str], name:str) -> pd.DataFrame:\n",
    "    logger.info(f\"Pipeline {name}: Cleaning numerical data...\")\n",
    "    for column in columns:\n",
    "        n_nans = X[column].isna().sum()\n",
    "        if n_nans > 0:\n",
    "            logger.info(f\"Column '{column}' has {n_nans} NaNs. Filling with mean {X[column].mean()}\")\n",
    "            X.loc[:, column] = X[column].fillna(X[column].mean())\n",
    "        \n",
    "    logger.info(f\"Pipeline {name}: Cleaning numerical data complete\")\n",
    "    return X\n",
    "\n",
    "\n",
    "def create_X_data_pipeline(\n",
    "    name: str, column_types: ColumnTypes, model: BaseEstimator | None = None\n",
    ") -> Pipeline:\n",
    "\n",
    "    cleaning_transformer = Pipeline(steps=[\n",
    "            (\n",
    "                \"cleaner_numerical\",\n",
    "                FunctionTransformer(clean_numerical, kw_args={\"name\": name, \"columns\": column_types[\"numerical\"]}),\n",
    "            ),\n",
    "            (\n",
    "                \"cleaner_categorical\",\n",
    "                FunctionTransformer(clean_categorical, kw_args={\"name\": name, \"columns\": column_types[\"categorical\"]}),\n",
    "            ),\n",
    "            (\n",
    "                \"label_encoder\",\n",
    "                FunctionTransformer(encode_label, kw_args={\"name\": name, \"columns\": column_types[\"labels\"]}),\n",
    "            ),  \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocessor_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"preprocessor_numerical\", StandardScaler(), column_types[\"numerical\"]),\n",
    "            (\n",
    "                \"preprocessor_categorical\",\n",
    "                OneHotEncoder(\n",
    "                    drop=None, sparse_output=False, handle_unknown=\"error\"\n",
    "                ),\n",
    "                column_types[\"categorical\"],\n",
    "            ),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "\n",
    "    pipeline_steps = [\n",
    "        (\"cleaning\", cleaning_transformer),\n",
    "        (\"preprocessing\", preprocessor_transformer),\n",
    "    ]\n",
    "\n",
    "    if model is not None:\n",
    "        pipeline_steps.append((f\"{name}_model\", model))\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        steps=pipeline_steps,\n",
    "        memory=Memory(\n",
    "            location=\"./cache/x\",\n",
    "            verbose=0,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def transform_target(X: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    logger.info(f\"Pipeline {name}: Transforming target...\")\n",
    "    y = pd.Series(X[\"class\"].apply(lambda el: 1.0 if el == \"e\" else 0.0)).rename(\"eadible\")\n",
    "    logger.info(f\"Pipeline {name}: Transforming target complete\")\n",
    "    return y\n",
    "\n",
    "def create_y_data_pipeline(name: str, column_types: ColumnTypes) -> Pipeline:\n",
    "\n",
    "    encoding_target_transformer = FunctionTransformer(transform_target, kw_args={\"name\": name})\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        steps=[(\"preprocessing\", encoding_target_transformer)],\n",
    "        memory=Memory(\n",
    "            location=\"./cache/y\",\n",
    "            verbose=0,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_accuracy(data: pd.DataFrame, raw_column_set: List[str], pipeline: Pipeline) -> float:\n",
    "    data = data.copy()\n",
    "    X = data[raw_column_set]\n",
    "    y = create_y_data_pipeline(\"y\", get_columns_types()).fit_transform(data)\n",
    "    \n",
    "    scores = cross_val_score(pipeline, X, y, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "    return scores.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_y = create_y_data_pipeline(\n",
    "#     name=\"dev_y\",\n",
    "#     column_types=column_types,\n",
    "# )\n",
    "# raw_transformed = pipeline_X.fit_transform(train.head(10000)[raw_column_set])\n",
    "# df_transformed = pd.DataFrame(data=raw_transformed, columns=pipeline_X.named_steps[\"preprocessing\"].get_feature_names_out())\n",
    "# raw_y = pipeline_y.fit_transform(train.head(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_column_set = [\"stem-height\", \"stem-width\", \"cap-diameter\", \"cap-shape\", \"cap-surface\", \"gill-color\", \"stem-root\", \"stem-surface\", \"stem-color\", \"veil-type\", \"veil-color\"]\n",
    "column_types = take_column_types_subset(get_columns_types(), raw_column_set, keep_ids_targets=True)\n",
    "pipeline_X = create_X_data_pipeline(\n",
    "    name=\"dev_x\",\n",
    "    column_types=column_types,\n",
    "    model=RandomForestClassifier(),\n",
    ")\n",
    "\n",
    "cv_score = get_cv_accuracy(train, raw_column_set, pipeline_X)\n",
    "display(f\"CV score: {cv_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
