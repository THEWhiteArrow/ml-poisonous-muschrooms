{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from typing import Tuple, List, Dict, cast\n",
    "from dataclasses import dataclass\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Memory\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train = pd.read_csv('./data/train.csv')\n",
    "    test = pd.read_csv('./data/test.csv')\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(name: str | None = None) -> logging.Logger:\n",
    "    \"\"\"A function to setup the logger\n",
    "\n",
    "    Args:\n",
    "        name (str | None, optional): Logger name. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        logging.Logger: Configured logger\n",
    "    \"\"\"\n",
    "\n",
    "    file_handler = logging.FileHandler(\"logs.log\", mode=\"a\")\n",
    "    stream_handler = logging.StreamHandler(stream=sys.stdout)\n",
    "\n",
    "    logger = logging.getLogger(name or __name__)\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.WARNING,\n",
    "        format=\"%(levelname)s %(name)s %(asctime)s | %(message)s\",\n",
    "        datefmt=\"%H:%M:%S\",\n",
    "        handlers=[file_handler, stream_handler],\n",
    "    )\n",
    "\n",
    "    \n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    return logger\n",
    "\n",
    "logger = setup_logger(\"Main\")\n",
    "logger.info(\"Logger setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ColumnTypes:\n",
    "    \"\"\"\n",
    "    A class to store the features and target columns of the dataset.\n",
    "\n",
    "    Attributes:\n",
    "    - ids: List[str]\n",
    "    - labels: List[str]\n",
    "    - numerical: List[str]\n",
    "    - categorical: List[str]\n",
    "    - targets: List[str]\n",
    "    \"\"\"\n",
    "\n",
    "    ids: List[str]\n",
    "    \"\"\"An array of the columns that are used as the ID of the dataset.\"\"\"\n",
    "    labels: List[str]\n",
    "    \"\"\"An array of the columns that are used as the labels of the dataset. \n",
    "    Label encoding is to be used when there is a sequential correlation between the labels.\"\"\"\n",
    "\n",
    "    numerical: List[str]\n",
    "    \"\"\"An array of the columns that are numerical in nature.\"\"\"\n",
    "    categorical: List[str]\n",
    "    \"\"\"An array of the columns that are categorical in nature.\"\"\"\n",
    "    targets: List[str]\n",
    "    \"\"\"An array of the columns that are the target of the dataset.\"\"\"   \n",
    "\n",
    "    def to_list(self) -> List[str]:\n",
    "        \"\"\"A function that returns a list of all the columns in the ColumnTypes object.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of all the columns in the ColumnTypes object.\n",
    "        \"\"\"\n",
    "        return self.ids + self.labels + self.numerical + self.categorical + self.targets\n",
    "\n",
    "\n",
    "    def filter(self, columns: List[str], keep_ids_targets : bool = False) -> \"ColumnTypes\":\n",
    "        \"\"\"A function that returns a subset of the ColumnTypes object.\n",
    "\n",
    "        Args:\n",
    "            columns (List[str]): A list of columns to be taken from the ColumnTypes object.\n",
    "\n",
    "        Returns:\n",
    "            ColumnTypes: A ColumnTypes object that contains the subset of columns.\n",
    "        \"\"\"\n",
    "        return ColumnTypes(\n",
    "            ids=self.ids if keep_ids_targets else [col for col in columns if col in self.ids],\n",
    "            labels=[col for col in columns if col in self.labels],\n",
    "            numerical=[col for col in columns if col in self.numerical],\n",
    "            categorical=[col for col in columns if col in self.categorical],\n",
    "            targets=self.targets if keep_ids_targets else [col for col in columns if col in self.targets],\n",
    "        )\n",
    "    \n",
    "    def adapt_names(self, columns: List[str]) -> \"ColumnTypes\":\n",
    "        \"\"\"A function that adapts the names of the columns in the ColumnTypes object.\n",
    "        The basic idea is that after the pipeline transformations, the names of the columns will be changed.\n",
    "        So we want to preserve the types of the original columns and updated them to any possibilites that include the original name in the new name.\n",
    "\n",
    "        Args:\n",
    "            columns (List[str]): A list of columns to be taken from the ColumnTypes object.\n",
    "\n",
    "        Returns:\n",
    "            ColumnTypes: A ColumnTypes object that contains the adapted names of the columns.\n",
    "        \"\"\"\n",
    "        \n",
    "        adaped_ids = [col for col in columns if any([f\"__{id}\" in col for id in self.ids])]\n",
    "        adaped_labels = [col for col in columns if any([f\"__{label}\" in col for label in self.labels])]\n",
    "        adaped_numerical = [col for col in columns if any([f\"__{numerical}\" in col for numerical in self.numerical])]\n",
    "        adaped_categorical = [col for col in columns if any([f\"__{categorical}\" in col for categorical in self.categorical])]\n",
    "        adaped_targets = [col for col in columns if any([f\"__{target}\" in col for target in self.targets])]\n",
    "\n",
    "        return ColumnTypes(\n",
    "            ids=adaped_ids,\n",
    "            labels=adaped_labels,\n",
    "            numerical=adaped_numerical,\n",
    "            categorical=adaped_categorical,\n",
    "            targets=adaped_targets\n",
    "        )\n",
    "        \n",
    "@dataclass\n",
    "class ColumnSet:\n",
    "    \"\"\"A class to store the information about the features of the dataset.\"\"\"\n",
    "\n",
    "    is_optional: bool\n",
    "    \"\"\"A boolean value to determine if the column list is optional.\"\"\"\n",
    "    columns: List[str]\n",
    "    \"\"\"An array of the columns that are to be used in the dataset.\"\"\"\n",
    "\n",
    "def get_possible_columns_configs(column_sets : List[ColumnSet]) -> List[List[str]]:\n",
    "    \"\"\"A function that returns the possible column configurations for the dataset.\n",
    "\n",
    "    Returns:\n",
    "        List[List[str]]: A list of lists that contain the possible column configurations.\n",
    "    \"\"\"\n",
    "    optional_columns_sets = [\n",
    "        column_set for column_set in column_sets if column_set.is_optional\n",
    "    ]\n",
    "    mandatory_columns_sets = [\n",
    "        column_set for column_set in column_sets if not column_set.is_optional\n",
    "    ]\n",
    "\n",
    "    if len(optional_columns_sets) > 10:\n",
    "        logger.warning(\n",
    "            f\"The number of optional columns sets is too high (more than 10) - {len(optional_columns_sets)}\"\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\n",
    "            f\"The number of optional columns sets is {len(optional_columns_sets)}\"\n",
    "        )\n",
    "\n",
    "    bitmap = 2 ** len(optional_columns_sets) - 1\n",
    "\n",
    "    possible_columns_configs: List[List[str]] = []\n",
    "    mandatory_columns: List[str] = [\n",
    "        column\n",
    "        for mandatory_set in mandatory_columns_sets\n",
    "        for column in mandatory_set.columns\n",
    "    ]\n",
    "    if len(mandatory_columns) > 0:\n",
    "        possible_columns_configs.append(mandatory_columns)\n",
    "        \n",
    "    for i in range(1, bitmap + 1):\n",
    "        columns_config: List[List[str]] = [*mandatory_columns]\n",
    "        for j in range(len(optional_columns_sets)):\n",
    "            if i & (1 << j):\n",
    "                columns_config.extend(optional_columns_sets[j].columns)\n",
    "\n",
    "        possible_columns_configs.append(columns_config)\n",
    "\n",
    "    return possible_columns_configs\n",
    "\n",
    "def verify_all_columns_types_exist(column_types: ColumnTypes, X: pd.DataFrame) -> bool:\n",
    "    \"\"\"A function that verifies if all the columns in the dataset exist in the ColumnTypes object.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): A pandas DataFrame that contains the features of the dataset.\n",
    "\n",
    "    Returns:\n",
    "        bool: A boolean value that determines if all the columns in the dataset exist in the ColumnTypes object.\n",
    "    \"\"\"\n",
    "    for column in X.columns:\n",
    "        if column not in column_types.ids + column_types.labels + column_types.numerical + column_types.categorical + column_types.targets:\n",
    "            logger.error(f\"Column {column} does not exist in the column types\")\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def verify_all_columns_sets_exist(column_sets: List[ColumnSet], X: pd.DataFrame) -> bool:\n",
    "    \"\"\"A function that verifies if all the columns in the dataset exist in the ColumnSets object.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): A pandas DataFrame that contains the features of the dataset.\n",
    "\n",
    "    Returns:\n",
    "        bool: A boolean value that determines if all the columns in the dataset exist in the ColumnSets object.\n",
    "    \"\"\"\n",
    "    for column_set in column_sets:\n",
    "        for column in column_set.columns:\n",
    "            if column not in X.columns:\n",
    "                logger.error(f\"Column {column} does not exist in the column sets\")\n",
    "                return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_types() -> ColumnTypes:\n",
    "    \"\"\"A function that defines the types of columns in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        ColumnTypes: ColumnTypes object that contains the columns of the dataset.\n",
    "    \"\"\"\n",
    "    return ColumnTypes(\n",
    "        ids=[\"id\"],\n",
    "        targets=[\"class\"],\n",
    "        labels=[\"gill-spacing\"],\n",
    "        numerical=[\n",
    "            \"cap-diameter\",\n",
    "            \"stem-height\",\n",
    "            \"stem-width\",\n",
    "        ],\n",
    "        categorical=[\n",
    "            \"cap-shape\",\n",
    "            \"cap-surface\",\n",
    "            \"cap-color\",\n",
    "            \"does-bruise-or-bleed\",\n",
    "            \"gill-attachment\",\n",
    "            \"gill-color\",\n",
    "            \"stem-root\",\n",
    "            \"stem-surface\",\n",
    "            \"stem-color\",\n",
    "            \"veil-type\",\n",
    "            \"veil-color\",\n",
    "            \"has-ring\",\n",
    "            \"ring-type\",\n",
    "            \"spore-print-color\",\n",
    "            \"habitat\",\n",
    "            \"season\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "def get_column_sets() -> List[ColumnSet]:\n",
    "    \"\"\"A function that defines the possible column combinations for the dataset.\n",
    "    It will create a list of ColumnSet objects that contain the possible column combinations.\n",
    "\n",
    "    Returns:\n",
    "        List[ColumnSet]: A list of ColumnSet objects that contain the possible column combinations.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        ColumnSet(is_optional=True, columns=[\"does-bruise-or-bleed\"]),\n",
    "        ColumnSet(is_optional=False, columns=[\"stem-height\", \"stem-width\"]),\n",
    "        ColumnSet(\n",
    "            is_optional=True,\n",
    "            columns=[\"cap-diameter\", \"cap-shape\", \"cap-surface\", \"cap-color\"],\n",
    "        ),\n",
    "        ColumnSet(\n",
    "            is_optional=True, columns=[\"gill-spacing\", \"gill-attachment\", \"gill-color\"]\n",
    "        ),\n",
    "        ColumnSet(\n",
    "            is_optional=True, columns=[\"stem-root\", \"stem-surface\", \"stem-color\"]\n",
    "        ),\n",
    "        ColumnSet(is_optional=True, columns=[\"veil-type\", \"veil-color\"]),\n",
    "        ColumnSet(is_optional=True, columns=[\"has-ring\", \"ring-type\"]),\n",
    "        ColumnSet(is_optional=True, columns=[\"spore-print-color\", \"habitat\", \"season\"]),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PIPELINES FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_categorical(X: pd.DataFrame, columns: List[str], name:str) -> pd.DataFrame:\n",
    "    \"\"\"A function that cleans the categorical data in the dataset.\n",
    "    It will remove the outliers (categories that are less than 1% of the total data) and replace them with \"nan\".\n",
    "    Additionally it will fill \"real\" NaNs with \"nan\" as well.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): _description_\n",
    "        columns (List[str]): _description_\n",
    "        name (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: _description_\n",
    "    \"\"\"\n",
    "    categorical_outliers_frequency_limit = 0.01\n",
    "    logger.info(f\"Pipeline {name}: Cleaning categorical data...\")\n",
    "    logger.info(\n",
    "        f\"Outliers frequency limit is {categorical_outliers_frequency_limit}\"\n",
    "    )\n",
    "\n",
    "    for column in columns:\n",
    "        value_counts = X[column].value_counts().to_frame()\n",
    "        sum_value_counts = value_counts[\"count\"].sum()\n",
    "\n",
    "        outliers = cast(\n",
    "                pd.DataFrame,\n",
    "                value_counts[\n",
    "                    value_counts[\"count\"]\n",
    "                    < sum_value_counts * categorical_outliers_frequency_limit\n",
    "                ],\n",
    "            ).index.to_list()\n",
    "        logger.info(f\"Outliers for column '{column}' are {outliers}\")\n",
    "\n",
    "        X.loc[:, column] = (\n",
    "            pd.Series(\n",
    "                X[column].apply(\n",
    "                    lambda el: el if el not in outliers else \"nan\"\n",
    "                )\n",
    "            )\n",
    "            .rename(column)\n",
    "            .fillna(\"nan\")\n",
    "            .astype(\"category\")\n",
    "        )\n",
    "\n",
    "    logger.info(f\"Pipeline {name}: Cleaning categorical data complete\")\n",
    "    return X\n",
    "\n",
    "def clean_numerical(X: pd.DataFrame, columns: List[str], name:str) -> pd.DataFrame:\n",
    "    logger.info(f\"Pipeline {name}: Cleaning numerical data...\")\n",
    "    for column in columns:\n",
    "        n_nans = X[column].isna().sum()\n",
    "        if n_nans > 0:\n",
    "            logger.info(f\"Column '{column}' has {n_nans} NaNs. Filling with mean {X[column].mean()}\")\n",
    "            X.loc[:, column] = X[column].fillna(X[column].mean())\n",
    "        \n",
    "    logger.info(f\"Pipeline {name}: Cleaning numerical data complete\")\n",
    "    return X\n",
    "\n",
    "def encode_label(X: pd.DataFrame, columns: List[str], name:str) -> pd.DataFrame:\n",
    "    logger.info(f\"Pipeline {name}: Encoding labels...\")\n",
    "    for column in columns:\n",
    "        X.loc[:, column] = pd.Series(LabelEncoder().fit_transform(X[column])).rename(column)\n",
    "\n",
    "    logger.info(f\"Pipeline {name}: Encoding labels complete\")\n",
    "    return X\n",
    "\n",
    "def encode_target(X: pd.DataFrame, name:str) -> pd.DataFrame:\n",
    "    logger.info(f\"Pipeline {name}: Encoding target...\")\n",
    "    X.loc[:, \"class\"] = X[\"class\"].apply(lambda el: 1.0 if el == \"e\" else 0.0)\n",
    "    logger.info(f\"Pipeline {name}: Encoding target complete\")\n",
    "    return X\n",
    "\n",
    "def convert_raw_pipeline_output_to_df(X: np.ndarray, pipeline: Pipeline, pipeline_step: str = \"column_transformer\") -> pd.DataFrame:\n",
    "    \"\"\"A function that converts the output of a pipeline to a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Raw pipeline output\n",
    "        pipeline (Pipeline): Pipeline that was used to transform the data\n",
    "        pipeline_step (str, optional): Steps from which the out features_names should be taken. Defaults to \"column_transformer\".\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame that contains the transformed data.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(f\"Converting raw pipeline output to DataFrame using pipeline_step: {pipeline_step}\")\n",
    "\n",
    "    if len(pipeline_step) == 0:\n",
    "        raise ValueError(\"Pipeline step cannot be empty\")\n",
    "    \n",
    "    pipeline_transformer = pipeline.named_steps[pipeline_step]\n",
    "    if not pipeline_transformer:\n",
    "        raise ValueError(f\"Pipeline step {pipeline_step} not found in the pipeline\")\n",
    "    \n",
    "    features_names = cast(ColumnTransformer, pipeline_transformer).get_feature_names_out()\n",
    "    \n",
    "    return pd.DataFrame(X, columns=features_names)\n",
    "\n",
    "def filter_columns(X: pd.DataFrame, columns: List[str], name:str) -> pd.DataFrame:\n",
    "    logger.info(f\"Pipeline {name}: Filtering columns...\")\n",
    "    X = X[columns]\n",
    "    logger.info(f\"Pipeline {name}: Filtering columns complete\")\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PIPELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessing_pipeline(column_types: ColumnTypes, name:str = 'preprocessing_default') -> Pipeline:\n",
    "    logger.info(\"Creating preprocessing pipeline...\")\n",
    "    \n",
    "    column_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"categorical\", OneHotEncoder(drop=None, sparse_output=False, handle_unknown=\"error\"), column_types.categorical),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "    preprocessing_pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"clean_numerical\", FunctionTransformer(clean_numerical, kw_args={\"columns\": column_types.numerical, \"name\": name})),\n",
    "            (\"clean_categorical\", FunctionTransformer(clean_categorical, kw_args={\"columns\": column_types.categorical, \"name\": name})),\n",
    "            (\"encode_labels\", FunctionTransformer(encode_label, kw_args={\"columns\": column_types.labels, \"name\": name})),\n",
    "            (\"encode_targets\", FunctionTransformer(encode_target, kw_args={\"name\": name})),\n",
    "            (\"column_transformer\", column_transformer),\n",
    "        ],\n",
    "        memory=Memory(location=f\"./cache/preprocessing/{name}\", verbose=0),\n",
    "    )\n",
    "    logger.info(\"Preprocessing pipeline created\")\n",
    "    return preprocessing_pipeline\n",
    "\n",
    "def create_scaling_filtering_pipeline(column_types: ColumnTypes, name:str = 'scaling_filtering_default') -> Pipeline:\n",
    "    logger.info(\"Creating scaling filtering pipeline...\")\n",
    "\n",
    "    filtered_columns : List[str] = list(set(column_types.to_list()) - set(column_types.ids) - set(column_types.targets))\n",
    "\n",
    "    column_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"scaled\", StandardScaler(), column_types.numerical),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "    scaling_pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"filter_transformer\", FunctionTransformer(filter_columns, kw_args={\"columns\": filtered_columns, \"name\": name})),\n",
    "            (\"column_transformer\", column_transformer),\n",
    "        ],\n",
    "        memory=None,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Scaling filtering pipeline created\")\n",
    "    return scaling_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    train, test = load_data()\n",
    "    column_types = get_columns_types()\n",
    "    column_sets = get_column_sets()\n",
    "    possible_columns_configs = get_possible_columns_configs(column_sets)\n",
    "\n",
    "    display(verify_all_columns_types_exist(column_types, train.head(10).copy()))\n",
    "    display(verify_all_columns_sets_exist(column_sets, train.head(10).copy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    preprocessing_pipeline = create_preprocessing_pipeline(column_types)\n",
    "\n",
    "    raw_preprocessed_data = preprocessing_pipeline.fit_transform(train.head(10*1000).copy())\n",
    "    df_preprocessed_data = convert_raw_pipeline_output_to_df(raw_preprocessed_data, preprocessing_pipeline)\n",
    "\n",
    "    display(df_preprocessed_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    dev_column_set = possible_columns_configs[5]\n",
    "    dev_column_types = column_types.filter(dev_column_set, keep_ids_targets=True)\n",
    "    preprocessed_column_names = df_preprocessed_data.columns.to_list()\n",
    "    display(dev_column_set)\n",
    "    display(dev_column_types)\n",
    "\n",
    "    adapted_column_types = dev_column_types.adapt_names(preprocessed_column_names)\n",
    "    display(adapted_column_types)\n",
    "\n",
    "    scaling_filtering_pipeline = create_scaling_filtering_pipeline(adapted_column_types)\n",
    "\n",
    "    raw_scaled_data = scaling_filtering_pipeline.fit_transform(df_preprocessed_data.copy())\n",
    "    df_scaled_data = convert_raw_pipeline_output_to_df(raw_scaled_data, scaling_filtering_pipeline)\n",
    "\n",
    "    display(df_scaled_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models() -> List[BaseEstimator]:\n",
    "    return [\n",
    "        RandomForestClassifier(),\n",
    "        SVC(),\n",
    "        KNeighborsClassifier(),\n",
    "        LogisticRegression(),\n",
    "        RidgeClassifier(),\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_config_accuracy(column_types : ColumnTypes, model: BaseEstimator, columns: List[str]) -> float:\n",
    "    \"\"\"A function that returns the accuracy of the model using cross-validation.\n",
    "\n",
    "    Args:\n",
    "        column_types (ColumnTypes): Defaul column types of the dataset\n",
    "        model (BaseEstimator): Model to be used for training\n",
    "        columns (List[str]): A list of columns to be used in the training\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model using cross-validation\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting the training process of model: {model.__class__.__name__} for columns: {column_set}\")\n",
    "\n",
    "    config_column_types = (\n",
    "        column_types\n",
    "            .filter(columns, keep_ids_targets=True)\n",
    "            .adapt_names(preprocessed_column_names)\n",
    "    )\n",
    "\n",
    "    scaling_filtering_pipeline = create_scaling_filtering_pipeline(config_column_types)\n",
    "\n",
    "    X = data.copy()\n",
    "    y = data.copy()[config_column_types.targets].squeeze()\n",
    "\n",
    "    scores = cross_val_score(\n",
    "        scaling_filtering_pipeline,\n",
    "        X,\n",
    "        y,\n",
    "        cv=5,\n",
    "        scoring=\"accuracy\",\n",
    "    )\n",
    "    acc = scores.mean()\n",
    "    logger.info(f\"Accuracy: {acc}\")\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = get_columns_types()\n",
    "column_sets = get_column_sets()\n",
    "possible_columns_configs = get_possible_columns_configs(column_sets)\n",
    "preprocessing_pipeline = create_preprocessing_pipeline(column_types)\n",
    "models = get_models()\n",
    "\n",
    "data = train.head(100*1000).copy()\n",
    "\n",
    "preprocessed_data = convert_raw_pipeline_output_to_df(preprocessing_pipeline.fit_transform(data), preprocessing_pipeline)\n",
    "preprocessed_column_names = preprocessed_data.columns.to_list() \n",
    "\n",
    "logger.info(\"Starting the test training process...\")\n",
    "logger.info(f\"Number of combinations: {len(possible_columns_configs) * len(models)}\")\n",
    "\n",
    "accuracies_df : pd.DataFrame = pd.DataFrame(columns=[\"model\", \"columns\", \"accuracy\"])\n",
    "\n",
    "for columns in possible_columns_configs:\n",
    "    for model in models:\n",
    "        acc = get_cv_config_accuracy(column_types, model, columns)\n",
    "\n",
    "        accuracies_df = accuracies_df.append(\n",
    "            {\n",
    "                \"model\": model.__class__.__name__,\n",
    "                \"columns\": columns,\n",
    "                \"accuracy\": acc,\n",
    "            },\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "display(accuracies_df.sort_values(by=\"accuracy\", ascending=False).head(10))\n",
    "\n",
    "accuracies_df.to_csv(f\"accuracies_{dt.datetime.now().isoformat()}.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
